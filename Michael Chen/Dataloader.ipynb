{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import pdb\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "\n",
    "#device = torch.device('cuda')\n",
    "\n",
    "torch.manual_seed(1117)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(1117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(root, path):\n",
    "    filenames = []\n",
    "    with ZipFile(root, 'r') as zfolder:\n",
    "        for filename in zfolder.namelist():\n",
    "            if filename.startswith(path + 'EOSINOPHIL'):\n",
    "                filenames.append([os.path.basename(filename), 0])\n",
    "            elif filename.startswith(path + 'LYMPHOCYTE'):\n",
    "                filenames.append([os.path.basename(filename), 1])\n",
    "            elif filename.startswith(path + 'MONOCYTE'):\n",
    "                filenames.append([os.path.basename(filename), 2])\n",
    "            elif filename.startswith(path + 'NEUTROPHIL'):\n",
    "                filenames.append([os.path.basename(filename), 3])\n",
    "    return pd.DataFrame(filenames, columns = ['filename', 'label'])\n",
    "\n",
    "# Source 1: https://thispointer.com/python-how-to-get-the-list-of-all-files-in-a-zip-archive/\n",
    "# Source 2: https://stackoverflow.com/questions/49625350/list-all-files-inside-a-folder-in-a-zip-file-in-python\n",
    "# Source 3: https://stackoverflow.com/questions/16091904/python-zip-how-to-eliminate-absolute-path-in-zip-archive-if-absolute-paths-for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data is ~20%, so I made the validate data ~20%\n",
    "def train_validate_split(df, train_size=0.75):\n",
    "    eosinophil = df.loc[df['label'] == 0]\n",
    "    etrain, evalidate = train_test_split(eosinophil, train_size = train_size, random_state = 1117)\n",
    "    \n",
    "    lymphocyte = df.loc[df['label'] == 1]\n",
    "    ltrain, lvalidate = train_test_split(lymphocyte, train_size = train_size, random_state = 1117)\n",
    "    \n",
    "    monocyte = df.loc[df['label'] == 2]\n",
    "    mtrain, mvalidate = train_test_split(monocyte, train_size = train_size, random_state = 1117)\n",
    "    \n",
    "    neutrophil = df.loc[df['label'] == 3]\n",
    "    ntrain, nvalidate = train_test_split(neutrophil, train_size = train_size, random_state = 1117)\n",
    "    \n",
    "    train = pd.concat([etrain, ltrain, mtrain, ntrain], axis = 0)\n",
    "    validate = pd.concat([evalidate, lvalidate, mvalidate, nvalidate], axis = 0)\n",
    "    \n",
    "    return train, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloodDataset(Dataset):\n",
    "    \"\"\"Blood Cell Images from https://www.kaggle.com/paultimothymooney/blood-cells.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.subfolder = ['EOSINOPHIL/', 'LYMPHOCYTE/', 'MONOCYTE/', 'NEUTROPHIL/']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sf = self.subfolder[self.data_frame['label'].iloc[idx]]\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir, sf, self.data_frame['filename'].iloc[idx])\n",
    "\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        # Added the following line to change image shape from (240, 320, 3) to (3, 240, 320)\n",
    "        #image = np.repeat(image[None,...],3,axis=0)\n",
    "        # Removed the above line, since it is not a clean transpose. Used following line instead.\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        \n",
    "        image = (image - image.mean()) / image.std()\n",
    "            \n",
    "        image_class = self.data_frame['label'].iloc[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {'x': image, 'y': image_class}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9957, 2), (7466, 2), (2491, 2), (2487, 2), (71, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root and partial_path are both based on unmodified data downloaded directly from source\n",
    "root = './blood-cells.zip'\n",
    "partial_path = 'dataset2-master/dataset2-master/images/'\n",
    "full_path = root + '/' + partial_path\n",
    "train_raw = make_df(root, partial_path + 'TRAIN/')\n",
    "train, validate = train_validate_split(train_raw)\n",
    "test = make_df(root, partial_path + 'TEST/')\n",
    "test_simple = make_df(root, partial_path + 'TEST_SIMPLE/')\n",
    "\n",
    "train_raw.shape, train.shape, validate.shape, test.shape, test_simple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train/validate/test sets as .csv\n",
    "train.to_csv('train.csv', sep=',', encoding='utf-8')\n",
    "validate.to_csv('validate.csv', sep=',', encoding='utf-8')\n",
    "test.to_csv('test.csv', sep=',', encoding='utf-8')\n",
    "test_simple.to_csv('test_simple.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = [(654 / 78), 1.0, (654/149)]\n",
    "# class_weights = torch.tensor(weights).to(device, dtype = torch.float)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# loss = torch.nn.CrossEntropyLoss(weight = class_weights)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 4\n",
    "\n",
    "dataset = {'train': BloodDataset('train.csv', full_path + 'TRAIN/'),\n",
    "           'validate': BloodDataset('validate.csv', full_path + 'TRAIN/'),\n",
    "           'test': BloodDataset('test.csv', full_path + 'TEST/'),\n",
    "           'test_simple': BloodDataset('test_simple.csv', full_path + 'TEST_SIMPLE')}\n",
    "dataloader = {x: DataLoader(dataset[x], batch_size = batch_size,\n",
    "                           shuffle = True, num_workers = 0) for x in ['train', 'validate', 'test', 'test_simple']}\n",
    "# lambda_func = lambda epoch: 0.5 ** epoch\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 240, 320]), torch.Size([4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure that our dataloader works as intended -- it does!\n",
    "next(iter(dataloader['train']))['x'].shape, next(iter(dataloader['train']))['y'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
